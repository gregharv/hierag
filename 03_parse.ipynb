{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebef51",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.482134+00:00"
   },
   "outputs": [],
   "source": [
    "from fastlite import database\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import re\n",
    "import importlib.util\n",
    "\n",
    "db = database('scraper.db')\n",
    "\n",
    "# Import utils\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"00_utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030a6e1",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.503000+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunks, discovered_urls, embeddings, extracts, pages, sites"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c42d0f",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.527935+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table pages (id, site_id, url, html, content_hash, last_scraped, last_changed)>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.t.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c5787",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.558388+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jea.com'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.t.pages(limit=1)[0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ac81a",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.591181+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.q(f\"select * from pages where site_id=2 limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cf91c",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.624938+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "<!DOCTYPE html>\r\n",
      "<html class=\"home\" lang=\"en\">\n"
     ]
    }
   ],
   "source": [
    "print(db.t.pages(limit=1)[0]['html'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ef2b4",
   "metadata": {
    "time_run": "2026-01-24T21:42:14.649143+00:00"
   },
   "outputs": [],
   "source": [
    "def extract_breadcrumb_context(html, site_id=None):\n",
    "    \"\"\"\n",
    "    Extract breadcrumb context from HTML.\n",
    "    Returns formatted context string like 'Context: Home > Residential > ...' or empty string.\n",
    "    \n",
    "    Args:\n",
    "        html: HTML content\n",
    "        site_id: Site ID to determine which selector to use (from utils.SITES config)\n",
    "    \"\"\"\n",
    "    if site_id is None:\n",
    "        return ''\n",
    "    \n",
    "    site_config = utils.get_site_config(site_id)\n",
    "    if not site_config or 'breadcrumb_selector' not in site_config:\n",
    "        return ''\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    breadcrumb_selector = site_config['breadcrumb_selector']\n",
    "    breadcrumb_element = soup.select_one(breadcrumb_selector)\n",
    "    \n",
    "    if not breadcrumb_element:\n",
    "        return ''\n",
    "    \n",
    "    # For JEA site (site_id=1), try to find list items first, otherwise extract text directly\n",
    "    if site_id == 1:\n",
    "        breadcrumb_items = breadcrumb_element.select('li')\n",
    "        if breadcrumb_items:\n",
    "            breadcrumb_texts = [li.get_text(strip=True) for li in breadcrumb_items if li.get_text(strip=True)]\n",
    "        else:\n",
    "            # Fallback: extract text and split by common separators\n",
    "            breadcrumb_text = breadcrumb_element.get_text(strip=True)\n",
    "            breadcrumb_texts = [t.strip() for t in breadcrumb_text.split('>') if t.strip()]\n",
    "    else:\n",
    "        # For other sites (like Connections), extract list items\n",
    "        breadcrumb_items = breadcrumb_element.select('li')\n",
    "        breadcrumb_texts = [li.get_text(strip=True) for li in breadcrumb_items if li.get_text(strip=True)]\n",
    "    \n",
    "    if breadcrumb_texts:\n",
    "        return 'Context: ' + ' > '.join(breadcrumb_texts) + '\\n\\n'\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8ce20",
   "metadata": {
    "time_run": "2026-01-24T21:44:11.797212+00:00"
   },
   "outputs": [],
   "source": [
    "def split_md_sections(html, selector, converter=None, min_len=100, max_len=1000):\n",
    "    \"\"\"\n",
    "    Split content by accordion items first, then by headers for remaining content.\n",
    "    max_len=6000 characters ensures chunks stay well under 8k token context limit.\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    container = soup.select_one(selector)\n",
    "    if not container:\n",
    "        return []\n",
    "\n",
    "    if converter is None:\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False\n",
    "        converter.body_width = 0\n",
    "\n",
    "    chunks = []\n",
    "    accordion_parts = []\n",
    "    \n",
    "    # Extract accordion items (looking for elements that might be accordions)\n",
    "    # This pattern looks for elements with \"Closed Title\" text or accordion-like structure\n",
    "    accordion_items = container.select('[class*=\"accordion\"], [class*=\"collapse\"], details')\n",
    "    \n",
    "    if accordion_items:\n",
    "        for item in accordion_items:\n",
    "            try:\n",
    "                # Check if element has a valid name before converting\n",
    "                if hasattr(item, 'name') and item.name:\n",
    "                    content = converter.handle(str(item)).strip()\n",
    "                    if content:\n",
    "                        # Check if it matches the accordion pattern in markdown\n",
    "                        if 'Accordion Item' in content or 'Closed Title' in content:\n",
    "                            accordion_parts.append(content)\n",
    "                            item.decompose()\n",
    "            except (TypeError, AttributeError, ValueError) as e:\n",
    "                # Skip malformed elements\n",
    "                continue\n",
    "    \n",
    "    if accordion_parts:\n",
    "        # Split accordion parts by the markdown pattern\n",
    "        for part in accordion_parts:\n",
    "            parts = re.split(r'\\n\\nAccordion Item\\n\\nClosed Title:', part)\n",
    "            for p in parts:\n",
    "                if p.strip(): chunks.append(p.strip())\n",
    "    \n",
    "    # Process remaining content\n",
    "    remaining = converter.handle(str(container)).strip()\n",
    "    if remaining:\n",
    "        for c in re.split(r'(?=^#{1,3}\\s)', remaining, flags=re.MULTILINE):\n",
    "            if c.strip(): chunks.append(c.strip())\n",
    "    \n",
    "    res = []\n",
    "    for c in chunks:\n",
    "        if res and len(res[-1]) < min_len: res[-1] += '\\n\\n' + c\n",
    "        elif len(c) > max_len:\n",
    "            paras = c.split('\\n\\n')\n",
    "            buf = ''\n",
    "            for p in paras:\n",
    "                if len(buf) + len(p) > max_len and buf:\n",
    "                    res.append(buf.strip())\n",
    "                    buf = p\n",
    "                else: buf += '\\n\\n' + p if buf else p\n",
    "            if buf: res.append(buf.strip())\n",
    "        else: res.append(c)\n",
    "    return res\n",
    "\n",
    "# page_html = db.q(f\"select html from pages where url='https://www.jea.com/my_account/rates/'\")[0]['html']\n",
    "# selector = db.q(f\"select selector from sites where id=1\")[0]['selector']\n",
    "# chunks = split_md_sections(page_html, selector)\n",
    "# [(i, len(c), c[:80]) for i,c in enumerate(chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eda5b0",
   "metadata": {
    "time_run": "2026-01-24T21:44:21.992500+00:00"
   },
   "outputs": [],
   "source": [
    "def split_with_tabs(html, selector, converter=None, min_len=100, max_len=1000):\n",
    "    \"\"\"\n",
    "    Split content by tabs first, then by headers for remaining content.\n",
    "    Used for extracting raw content before creating extracts.\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    container = soup.select_one(selector)\n",
    "    if not container:\n",
    "        return []\n",
    "\n",
    "    if converter is None:\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False\n",
    "        converter.body_width = 0\n",
    "\n",
    "    tabs_wrap = container.select_one('.kt-tabs-content-wrap')\n",
    "    tabs_list = container.select_one('.kt-tabs-title-list')\n",
    "    intro = container.select_one('.doc-content-wrap > p')\n",
    "    \n",
    "    chunks = []\n",
    "    step_parts = []\n",
    "    \n",
    "    if intro:\n",
    "        step_parts.append(intro.get_text().strip())\n",
    "        intro.decompose()\n",
    "    \n",
    "    if tabs_wrap:\n",
    "        for tab in tabs_wrap.select('[class*=\"kt-inner-tab-\"]'):\n",
    "            # Extract step number from class name (e.g., \"kt-inner-tab-1\" -> \"1\")\n",
    "            classes = tab.get('class', [])\n",
    "            step_num = ''\n",
    "            for cls in classes:\n",
    "                if cls.startswith('kt-inner-tab-'):\n",
    "                    step_num = cls.replace('kt-inner-tab-', '')\n",
    "                    break\n",
    "            # Fallback: try to extract from class string if class list doesn't work\n",
    "            if not step_num:\n",
    "                class_str = ' '.join(classes) if classes else str(tab.get('class', ''))\n",
    "                match = re.search(r'kt-inner-tab-(\\d+)', class_str)\n",
    "                if match:\n",
    "                    step_num = match.group(1)\n",
    "            content = converter.handle(str(tab)).strip()\n",
    "            if step_num and content: step_parts.append(f\"Step {step_num}: {content}\")\n",
    "        tabs_wrap.decompose()\n",
    "    \n",
    "    if tabs_list: tabs_list.decompose()\n",
    "    if step_parts: chunks.append('\\n\\n'.join(step_parts))\n",
    "    \n",
    "    remaining = converter.handle(str(container)).strip()\n",
    "    if remaining:\n",
    "        for c in re.split(r'(?=^#{1,3}\\s)', remaining, flags=re.MULTILINE):\n",
    "            if c.strip(): chunks.append(c.strip())\n",
    "    \n",
    "    res = []\n",
    "    for c in chunks:\n",
    "        if res and len(res[-1]) < min_len: res[-1] += '\\n\\n' + c\n",
    "        elif len(c) > max_len:\n",
    "            paras = c.split('\\n\\n')\n",
    "            buf = ''\n",
    "            for p in paras:\n",
    "                if len(buf) + len(p) > max_len and buf:\n",
    "                    res.append(buf.strip())\n",
    "                    buf = p\n",
    "                else: buf += '\\n\\n' + p if buf else p\n",
    "            if buf: res.append(buf.strip())\n",
    "        else: res.append(c)\n",
    "    \n",
    "    return res\n",
    "\n",
    "# page = db.q(f\"select * from pages where site_id=2 limit 1\")[0]\n",
    "# site = db.q(f\"select * from sites where id=2\")[0]\n",
    "# chunks2 = split_with_tabs(page['html'], site['selector'])\n",
    "# [(i, len(c), c[:80]) for i,c in enumerate(chunks2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1c3c6",
   "metadata": {
    "time_run": "2026-01-24T21:44:31.271202+00:00"
   },
   "outputs": [],
   "source": [
    "# print(chunks2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eb542",
   "metadata": {
    "time_run": "2026-01-24T21:44:34.429738+00:00"
   },
   "outputs": [],
   "source": [
    "def create_extracts_from_page(html, selector, site_id, max_extract_len=100_000):\n",
    "    \"\"\"\n",
    "    Create extracts from a page HTML.\n",
    "    Extracts are 10,000 character chunks with breadcrumb context prepended.\n",
    "    \n",
    "    Returns:\n",
    "        List of extract text strings with context\n",
    "    \"\"\"\n",
    "    # Extract breadcrumb context\n",
    "    context_prefix = extract_breadcrumb_context(html, site_id)\n",
    "    \n",
    "    # Get raw content based on site_id configuration\n",
    "    site_config = utils.get_site_config(site_id)\n",
    "    if site_config and 'split_function' in site_config:\n",
    "        split_func_name = site_config['split_function']\n",
    "        if split_func_name == 'split_with_tabs':\n",
    "            raw_chunks = split_with_tabs(html, selector, max_len=100_000)\n",
    "        elif split_func_name == 'split_md_sections':\n",
    "            raw_chunks = split_md_sections(html, selector, max_len=100_000)\n",
    "        else:\n",
    "            # Fallback: simple HTML to text conversion\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            container = soup.select_one(selector)\n",
    "            if not container:\n",
    "                return []\n",
    "            converter = html2text.HTML2Text()\n",
    "            converter.ignore_links = False\n",
    "            converter.body_width = 0\n",
    "            raw_chunks = [converter.handle(str(container)).strip()]\n",
    "    else:\n",
    "        # Fallback: simple HTML to text conversion\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        container = soup.select_one(selector)\n",
    "        if not container:\n",
    "            return []\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False\n",
    "        converter.body_width = 0\n",
    "        raw_chunks = [converter.handle(str(container)).strip()]\n",
    "    \n",
    "    # Combine chunks and split into extracts of max_extract_len\n",
    "    full_text = '\\n\\n'.join(raw_chunks)\n",
    "    \n",
    "    extracts = []\n",
    "    if full_text:\n",
    "        # Split into extracts of max_extract_len, trying to break at paragraph boundaries\n",
    "        while len(full_text) > max_extract_len:\n",
    "            # Try to find a good break point (double newline)\n",
    "            break_point = max_extract_len\n",
    "            last_break = full_text.rfind('\\n\\n', 0, max_extract_len)\n",
    "            if last_break > max_extract_len * 0.5:  # Only use if it's not too early\n",
    "                break_point = last_break + 2\n",
    "            \n",
    "            extract_text = full_text[:break_point].strip()\n",
    "            if extract_text:\n",
    "                extracts.append(extract_text)\n",
    "            full_text = full_text[break_point:].strip()\n",
    "        \n",
    "        # Add remaining text\n",
    "        if full_text:\n",
    "            extracts.append(full_text)\n",
    "    \n",
    "    # Prepend context to all extracts\n",
    "    if context_prefix:\n",
    "        extracts = [context_prefix + extract for extract in extracts]\n",
    "    \n",
    "    return extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabef71",
   "metadata": {
    "time_run": "2026-01-24T21:44:34.980646+00:00"
   },
   "outputs": [],
   "source": [
    "def create_chunks_from_extract(extract_text, max_chunk_len=1000):\n",
    "    \"\"\"\n",
    "    Create chunks from an extract text.\n",
    "    Chunks are 1,000 character pieces with the same context as the extract.\n",
    "    \n",
    "    Args:\n",
    "        extract_text: The extract text (already includes context prefix)\n",
    "        max_chunk_len: Maximum length for each chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk text strings\n",
    "    \"\"\"\n",
    "    # Extract context prefix if present\n",
    "    context_prefix = ''\n",
    "    if extract_text.startswith('Context:'):\n",
    "        # Find where context ends (after the first double newline)\n",
    "        context_end = extract_text.find('\\n\\n', 0)\n",
    "        if context_end > 0:\n",
    "            context_prefix = extract_text[:context_end + 2]\n",
    "            extract_text = extract_text[context_end + 2:].strip()\n",
    "    \n",
    "    chunks = []\n",
    "    if extract_text:\n",
    "        # Split into chunks of max_chunk_len, prioritizing markdown header boundaries\n",
    "        while len(extract_text) > max_chunk_len:\n",
    "            break_point = max_chunk_len\n",
    "            \n",
    "            # First, try to find a markdown header (line starting with #) before max_chunk_len\n",
    "            # Search backwards from max_chunk_len to find the nearest header\n",
    "            search_end = min(len(extract_text), max_chunk_len)\n",
    "            search_start = max(0, max_chunk_len - 1000)  # Search back up to 1000 chars\n",
    "            search_text = extract_text[search_start:search_end]\n",
    "            \n",
    "            # Find all markdown headers in the search area (headers start at beginning of line)\n",
    "            # Pattern matches: start of line, then # followed by 1-6 more #, then space\n",
    "            header_pattern = re.compile(r'^#{1,6}\\s+', re.MULTILINE)\n",
    "            header_matches = list(header_pattern.finditer(search_text))\n",
    "            \n",
    "            if header_matches:\n",
    "                # Use the last (closest to max_chunk_len) header as break point\n",
    "                # Split BEFORE the header so header stays with its content\n",
    "                last_header_match = header_matches[-1]\n",
    "                header_pos = search_start + last_header_match.start()\n",
    "                if header_pos > max_chunk_len * 0.3:  # Only use if not too early\n",
    "                    break_point = header_pos\n",
    "            else:\n",
    "                # Fallback: try to find a paragraph boundary (double newline)\n",
    "                last_break = extract_text.rfind('\\n\\n', 0, max_chunk_len)\n",
    "                if last_break > max_chunk_len * 0.5:  # Only use if it's not too early\n",
    "                    break_point = last_break + 2\n",
    "            \n",
    "            chunk_text = extract_text[:break_point].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append(chunk_text)\n",
    "            extract_text = extract_text[break_point:].strip()\n",
    "        \n",
    "        # Add remaining text\n",
    "        if extract_text:\n",
    "            chunks.append(extract_text)\n",
    "    \n",
    "    # Prepend context to all chunks\n",
    "    if context_prefix:\n",
    "        chunks = [context_prefix + chunk for chunk in chunks]\n",
    "    else:\n",
    "        chunks = [chunk for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3fa66",
   "metadata": {
    "time_run": "2026-01-24T21:44:38.616001+00:00"
   },
   "outputs": [],
   "source": [
    "def process_all_pages_to_extracts_and_chunks(db):\n",
    "    \"\"\"\n",
    "    Process all pages to create extracts (10,000 chars) and chunks (1,000 chars).\n",
    "    Both extracts and chunks include breadcrumb context.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (num_extracts_created, num_chunks_created)\n",
    "    \"\"\"\n",
    "    all_pages = list(db.t.pages())\n",
    "    total_extracts = 0\n",
    "    total_chunks = 0\n",
    "    \n",
    "    for page in all_pages:\n",
    "        site_id = page['site_id']\n",
    "        site = db.t.sites[site_id]\n",
    "        \n",
    "        if not site:\n",
    "            print(f\"Warning: Site {site_id} not found for page {page['id']}\")\n",
    "            continue\n",
    "        \n",
    "        # Create extracts from page\n",
    "        extracts = create_extracts_from_page(page['html'], site['selector'], site_id, max_extract_len=100_000)\n",
    "        \n",
    "        # Store extracts in database\n",
    "        page_chunks_count = 0\n",
    "        for i, extract_text in enumerate(extracts):\n",
    "            extract = db.t.extracts.insert(\n",
    "                page_id=page['id'],\n",
    "                extract_index=i,\n",
    "                text=extract_text,\n",
    "            )\n",
    "            total_extracts += 1\n",
    "            \n",
    "            # Create chunks from extract\n",
    "            chunks = create_chunks_from_extract(extract_text, max_chunk_len=1000)\n",
    "            \n",
    "            # Store chunks in database\n",
    "            for j, chunk_text in enumerate(chunks):\n",
    "                db.t.chunks.insert(\n",
    "                    extract_id=extract['id'],\n",
    "                    chunk_index=j,\n",
    "                    text=chunk_text,\n",
    "                )\n",
    "                total_chunks += 1\n",
    "                page_chunks_count += 1\n",
    "        \n",
    "        print(f\"Page {page['id']}: Created {len(extracts)} extracts, {page_chunks_count} chunks\")\n",
    "    \n",
    "    return total_extracts, total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93fadbd",
   "metadata": {
    "time_run": "2026-01-24T21:44:41.742162+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Created 0 extracts, 0 chunks\n",
      "Page 2: Created 1 extracts, 9 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3: Created 1 extracts, 14 chunks\n",
      "Page 4: Created 1 extracts, 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5: Created 1 extracts, 14 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 6: Created 1 extracts, 3 chunks\n",
      "Page 7: Created 0 extracts, 0 chunks\n",
      "Page 8: Created 1 extracts, 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9: Created 1 extracts, 6 chunks\n",
      "Page 10: Created 0 extracts, 0 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 11: Created 1 extracts, 31 chunks\n",
      "\n",
      "Total: Created 8 extracts and 82 chunks\n"
     ]
    }
   ],
   "source": [
    "# Process all pages to create extracts and chunks\n",
    "extracts_count, chunks_count = process_all_pages_to_extracts_and_chunks(db)\n",
    "print(f\"\\nTotal: Created {extracts_count} extracts and {chunks_count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa79b4f",
   "metadata": {
    "time_run": "2026-01-24T21:44:47.802766+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample extract (ID: 1):\n",
      "Length: 7566 chars\n",
      "Preview: Context: Home > Outage Center > Outage FAQ\n",
      "\n",
      "Accordion Item\n",
      "\n",
      "Closed Title:Report an Electric Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * To report an issue with your electric service, [sign into your account](/Apps/Outage/...\n",
      "\n",
      "Chunks for this extract: 9\n",
      "First chunk length: 909 chars\n",
      "First chunk preview: Context: Home > Outage Center > Outage FAQ\n",
      "\n",
      "Accordion Item\n",
      "\n",
      "Closed Title:Report an Electric Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * To report an issue with your electric service, [sign into your account](/Apps/Outage/...\n"
     ]
    }
   ],
   "source": [
    "# Verify the structure\n",
    "if db.t.extracts():\n",
    "    sample_extract = list(db.t.extracts(limit=1))[0]\n",
    "    print(f\"\\nSample extract (ID: {sample_extract['id']}):\")\n",
    "    print(f\"Length: {len(sample_extract['text'])} chars\")\n",
    "    print(f\"Preview: {sample_extract['text'][:200]}...\")\n",
    "    \n",
    "    # Show chunks for this extract\n",
    "    extract_chunks = list(db.t.chunks.rows_where('extract_id=?', [sample_extract['id']]))\n",
    "    print(f\"\\nChunks for this extract: {len(extract_chunks)}\")\n",
    "    if extract_chunks:\n",
    "        print(f\"First chunk length: {len(extract_chunks[0]['text'])} chars\")\n",
    "        print(f\"First chunk preview: {extract_chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456c3ad",
   "metadata": {
    "time_run": "2026-01-24T21:45:03.298897+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Home > Outage Center > Outage FAQ\n",
      "\n",
      "Accordion Item\n",
      "\n",
      "Closed Title:Report an Electric Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * To report an issue with your electric service, [sign into your account](/Apps/Outage/ReportProblem.aspx \"Sign in to report an outage\").\n",
      "  * Select the account associated with the issue or outage.\n",
      "  * Select Electric service.\n",
      "  * Select the service issue from the “The Problem is” dropdown menu\n",
      "  * Any additional comments that might help remedy the issue.\n",
      "  * Click Report Issue.\n",
      "\n",
      "Track an Electric Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * To track a previously reported issue or outage, [sign into your account](/Login.aspx?ReturnUrl=/Apps/Outage/ReportProblem.aspx \"Sign in to track an outage\"). \n",
      "  * Select the account associated with the outage.\n",
      "  * Select Electric service.\n",
      "  * If you already have a ticket reported for the account and service type, the status will appear.\n",
      "\n",
      "Report a Water Issue\n",
      "\n",
      "Open Text:\n"
     ]
    }
   ],
   "source": [
    "print(db.t.chunks()[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b0529",
   "metadata": {
    "time_run": "2026-01-24T21:45:10.335782+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Home > Outage Center > Outage FAQ\n",
      "\n",
      "* To report an issue with your water service, [sign into your account](/Login.aspx?ReturnUrl=/Apps/Outage/ReportProblem.aspx \"Sign in to report an outage\").\n",
      "  * Select the account associated with the issue or outage \n",
      "  * Select Water service. \n",
      "  * Select the service issue from the “The Problem is” dropdown menu.\n",
      "  * Add any additional comments that might help remedy the issue.\n",
      "  * Click Report Issue.\n",
      "\n",
      "Track a Water Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * To track a previously reported issue or outage, [sign into your account](/Apps/Outage/ReportProblem.aspx \"Sign in to track an outage\").\n",
      "  * Select the account associated with the outage. \n",
      "  * Select Water service.\n",
      "  * If you already have a ticket reported for the account and service type, the status will appear.\n",
      "\n",
      "Accordion Item\n",
      "\n",
      "Closed Title:Report an Electric Issue\n",
      "\n",
      "Open Text:\n"
     ]
    }
   ],
   "source": [
    "print(db.t.chunks()[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dda58f",
   "metadata": {
    "time_run": "2026-01-24T21:45:20.000848+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Home > Outage Center > Outage FAQ\n",
      "\n",
      "* [Visit the Outage Map](/Outage_Center/Outage_Map/ \"Report an outage\")\n",
      "  * Click the red “Report Electric Outage” button. \n",
      "  * Provide the information of the account linked to the outage (account number or phone number and house number or zip code). \n",
      "  * Confirm the location of the outage. \n",
      "  * Click Submit.\n",
      "\n",
      "Track an Electric Issue\n",
      "\n",
      "Open Text:\n",
      "\n",
      "  * [Visit the Outage Map](/Outage_Center/Outage_Map/ \"Report an outage\")\n",
      "  * Find your location on the map.\n",
      "  * Click the circular icon over your outage location.\n",
      "  * The Outage Information panel will show you the status of the outage, estimated restoration time (if one has been established), and the cause as information is available.\n",
      "\n",
      "Accordion Item\n",
      "\n",
      "Closed Title:Who gets restored first?\n",
      "\n",
      "Open Text:\n"
     ]
    }
   ],
   "source": [
    "print(db.t.chunks()[2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b9d1b",
   "metadata": {
    "time_run": "2026-01-24T21:42:15.393758+00:00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe52f42",
   "metadata": {
    "time_run": "2026-01-24T21:42:15.419744+00:00"
   },
   "outputs": [],
   "source": [
    "# Clear existing extracts and chunks\n",
    "# for x in db.t.chunks():\n",
    "#     db.t.chunks.delete(x['id'])\n",
    "\n",
    "# for x in db.t.extracts():\n",
    "#     db.t.extracts.delete(x['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee456ba",
   "metadata": {
    "time_run": "2026-01-24T21:42:15.446398+00:00"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
