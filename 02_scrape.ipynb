{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdf62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastlite import database\n",
    "\n",
    "db = database('scraper.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87597c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunks, discovered_urls, embeddings, extracts, pages, sites"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a6b87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'root_url': 'https://www.jea.com',\n",
       "  'selector': '#secondary-content > div > div.cb-content-container.cf',\n",
       "  'name': 'JEA'},\n",
       " {'id': 2,\n",
       "  'root_url': 'https://connections',\n",
       "  'selector': '#post > div.doc-scrollable.editor-content',\n",
       "  'name': 'connections'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.q(f\"select * from sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddf4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'site_id': 1,\n",
       "  'url': 'https://www.jea.com',\n",
       "  'discovered_at': '2026-01-23T20:55:58.624022'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.t.discovered_urls(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fffdef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_discovered_pages(db, site_id=None, url_filter=None, delay=0.5):\n",
    "    \"\"\"\n",
    "    Scrape pages from discovered_urls table and store them in pages table.\n",
    "    \n",
    "    Args:\n",
    "        db: Database connection\n",
    "        site_id: Optional site_id to filter by. If None, scrapes all sites.\n",
    "        url_filter: Optional callable that takes a URL and returns True if it should be scraped.\n",
    "        delay: Delay between requests in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        Number of pages scraped.\n",
    "    \"\"\"\n",
    "    import httpx\n",
    "    import hashlib\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    \n",
    "    # Find URLs from discovered_urls that haven't been scraped yet (not in pages table)\n",
    "    if site_id:\n",
    "        query = \"\"\"\n",
    "            SELECT d.id, d.site_id, d.url \n",
    "            FROM discovered_urls d\n",
    "            LEFT JOIN pages p ON d.url = p.url\n",
    "            WHERE d.site_id=? AND p.id IS NULL\n",
    "        \"\"\"\n",
    "        params = (site_id,)\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "            SELECT d.id, d.site_id, d.url \n",
    "            FROM discovered_urls d\n",
    "            LEFT JOIN pages p ON d.url = p.url\n",
    "            WHERE p.id IS NULL\n",
    "        \"\"\"\n",
    "        params = ()\n",
    "    \n",
    "    pages_to_scrape = db.execute(query, params).fetchall()\n",
    "    \n",
    "    if url_filter:\n",
    "        pages_to_scrape = [(pid, sid, url) for pid, sid, url in pages_to_scrape if url_filter(url)]\n",
    "    \n",
    "    scraped = 0\n",
    "    for discovered_id, site_id_val, url in pages_to_scrape:\n",
    "        try:\n",
    "            resp = httpx.get(url, timeout=10, follow_redirects=True, verify=False)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"✗ {url}: status {resp.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            html = resp.text\n",
    "            content_hash = hashlib.md5(html.encode()).hexdigest()\n",
    "            now = datetime.utcnow().isoformat()\n",
    "            \n",
    "            # Check if we already have this content_hash (duplicate HTML)\n",
    "            existing_with_hash = list(db.t.pages.rows_where('content_hash=?', [content_hash], limit=1))\n",
    "            \n",
    "            if existing_with_hash:\n",
    "                # Duplicate HTML found - reuse the existing HTML to avoid storing duplicate\n",
    "                existing_page = existing_with_hash[0]\n",
    "                existing_html = existing_page['html']\n",
    "                print(f\"⊘ {url} (duplicate HTML, reusing from {existing_page['url']})\")\n",
    "                # Still insert a page record for this URL, but reuse the HTML\n",
    "                db.t.pages.insert(site_id=site_id_val, url=url, html=existing_html, content_hash=content_hash,\n",
    "                                 last_scraped=now, last_changed=existing_page.get('last_changed', now))\n",
    "            else:\n",
    "                # New unique HTML content\n",
    "                db.t.pages.insert(site_id=site_id_val, url=url, html=html, content_hash=content_hash,\n",
    "                                 last_scraped=now, last_changed=now)\n",
    "                print(f\"✓ {url} (scraped)\")\n",
    "                scraped += 1\n",
    "            \n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {url}: {e}\")\n",
    "    \n",
    "    return scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f7a62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_page(db, site_id, url):\n",
    "    \"\"\"Fetch a single page and store it\"\"\"\n",
    "    site = db.t.sites[site_id]\n",
    "    if not site: raise ValueError(f\"No site with id {site_id}\")\n",
    "    \n",
    "    resp = httpx.get(url, timeout=10, follow_redirects=True, verify=False)\n",
    "    html = resp.text\n",
    "    content_hash = hashlib.md5(html.encode()).hexdigest()\n",
    "    now = datetime.utcnow().isoformat()\n",
    "    \n",
    "    # Check if URL already exists\n",
    "    existing_by_url = list(db.t.pages.rows_where('url=?', [url], limit=1))\n",
    "    \n",
    "    if existing_by_url:\n",
    "        page = existing_by_url[0]\n",
    "        # Check if content changed\n",
    "        if page['content_hash'] != content_hash:\n",
    "            db.t.pages.update({'id': page['id'], 'html': html, 'content_hash': content_hash,\n",
    "                              'last_scraped': now, 'last_changed': now})\n",
    "            print(f\"↻ Updated: {url}\")\n",
    "        else:\n",
    "            db.t.pages.update({'id': page['id'], 'last_scraped': now})\n",
    "            print(f\"  {url} (unchanged)\")\n",
    "        return page['id']\n",
    "    else:\n",
    "        # Check if we already have this content_hash (duplicate HTML)\n",
    "        existing_with_hash = list(db.t.pages.rows_where('content_hash=?', [content_hash], limit=1))\n",
    "        \n",
    "        if existing_with_hash:\n",
    "            # Duplicate HTML found - reuse the existing HTML\n",
    "            existing_page = existing_with_hash[0]\n",
    "            existing_html = existing_page['html']\n",
    "            print(f\"⊘ {url} (duplicate HTML, reusing from {existing_page['url']})\")\n",
    "            row = db.t.pages.insert(site_id=site_id, url=url, html=existing_html, content_hash=content_hash,\n",
    "                                    last_scraped=now, last_changed=existing_page.get('last_changed', now))\n",
    "        else:\n",
    "            # New unique HTML content\n",
    "            row = db.t.pages.insert(site_id=site_id, url=url, html=html, content_hash=content_hash,\n",
    "                                    last_scraped=now, last_changed=now)\n",
    "            print(f\"✓ New: {url}\")\n",
    "        return row['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b126f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harvgs-admin\\AppData\\Local\\Temp\\4\\ipykernel_19604\\2877083102.py:52: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com (scraped)\n",
      "✓ https://www.jea.com/About/Procurement/Look_Up_an_Invoice/ (scraped)\n",
      "✓ https://www.jea.com/About/Careers/Culture/ (scraped)\n",
      "✓ https://www.jea.com/Business_Resources/Industrial_Pretreatment/Pretreatment_Program,_Permits,_Surveys_and_Applications/ (scraped)\n",
      "✓ https://www.jea.com/Manage_My_Account/Online_Bill_Payment/ (scraped)\n",
      "✓ https://www.jea.com/About/Board_and_Management/ (scraped)\n",
      "✓ https://www.jea.com/My_Account/Residential_Forms_and_Policies/JEA_Credit_Score/ (scraped)\n",
      "✓ https://www.jea.com/My_Account/Understand_My_Bill/High_Bills/Electric_Spikes/ (scraped)\n",
      "✓ https://www.jea.com/About/Procurement/Contractor_Safety/Intrinsically_Hazardous_Work/ (scraped)\n",
      "✓ https://www.jea.com/My_Account/Residential_Forms_and_Policies/Name_Changes_and_Account_Transfers/ (scraped)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_discovered_pages(db, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1834858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New: https://www.jea.com/my_account/rates/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harvgs-admin\\AppData\\Local\\Temp\\4\\ipykernel_19604\\2159609341.py:14: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JEA page (site_id=1)\n",
    "fetch_page(db, 1, 'https://www.jea.com/my_account/rates/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3baa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ New: https://connections/?docs=residential/start-stop-transfer-traditional-service/transfer-service/transferring-service\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harvgs-admin\\AppData\\Local\\Temp\\4\\ipykernel_19604\\2159609341.py:14: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connections page (site_id=2) \n",
    "fetch_page(db, 2, 'https://connections/?docs=residential/start-stop-transfer-traditional-service/transfer-service/transferring-service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a6c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
