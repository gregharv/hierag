{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11f72a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunks, discovered_urls, embeddings, extracts, pages, sites"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastlite import database\n",
    "\n",
    "db = database('scraper.db')\n",
    "\n",
    "sites = db.t.sites\n",
    "if sites not in db.t:\n",
    "    sites.create(id=int, root_url=str, selector=str, name=str, pk='id')\n",
    "    sites.create_index(['root_url'], unique=True)\n",
    "\n",
    "discovered_urls = db.t.discovered_urls\n",
    "if discovered_urls not in db.t:\n",
    "    discovered_urls.create(id=int, site_id=int, url=str, discovered_at=str, pk='id', foreign_keys=[('site_id', 'sites')])\n",
    "    discovered_urls.create_index(['url'], unique=True)\n",
    "\n",
    "pages = db.t.pages\n",
    "if pages not in db.t:\n",
    "    pages.create(id=int, site_id=int, url=str, html=str, content_hash=str, last_scraped=str, last_changed=str, pk='id', foreign_keys=[('site_id', 'sites')])\n",
    "    pages.create_index(['url'], unique=True)\n",
    "\n",
    "extracts = db.t.extracts\n",
    "if extracts not in db.t:\n",
    "    extracts.create(id=int, page_id=int, extract_index=int, text=str, pk='id', foreign_keys=[('page_id', 'pages')])\n",
    "\n",
    "chunks = db.t.chunks\n",
    "if chunks not in db.t:\n",
    "    chunks.create(id=int, extract_id=int, chunk_index=int, text=str, pk='id', foreign_keys=[('extract_id', 'extracts')])\n",
    "\n",
    "embeddings = db.t.embeddings\n",
    "if embeddings not in db.t:\n",
    "    embeddings.create(id=int, chunk_id=int, embedding=bytes, pk='id', foreign_keys=[('chunk_id', 'chunks')])\n",
    "\n",
    "db.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b8fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'root_url': 'https://connections',\n",
       " 'selector': '#post > div.doc-scrollable.editor-content',\n",
       " 'name': 'connections'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"00_utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "dt = db.t\n",
    "for site in utils.SITES:\n",
    "    dt.sites.insert(**site, ignore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0617c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "def get_internal_links(soup, base_url, root_url):\n",
    "    \"\"\"Extract all internal links from a page that belong to the same domain as root_url.\"\"\"\n",
    "    root_parsed = urlparse(root_url)\n",
    "    root_netloc, root_scheme = root_parsed.netloc, root_parsed.scheme or 'https'\n",
    "    netloc_variants = {root_netloc, '', root_netloc[4:] if root_netloc.startswith('www.') else f'www.{root_netloc}'}\n",
    "    \n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        parsed = urlparse(urljoin(base_url, a['href']))\n",
    "        if parsed.netloc in netloc_variants:\n",
    "            clean_url = f\"{root_scheme}://{root_netloc}{parsed.path}\"\n",
    "            if not clean_url.endswith(('.pdf', '.jpg', '.png', '.gif', '.zip')):\n",
    "                links.add(clean_url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666662df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def crawl_site(db, site_id, max_pages=10, delay=0.5):\n",
    "    \"\"\"Crawl a site to discover URLs. Only stores URLs, not HTML content.\"\"\"\n",
    "    site = db.t.sites[site_id]\n",
    "    if not site: raise ValueError(f\"No site with id {site_id}\")\n",
    "    root_url, domain = site['root_url'], urlparse(site['root_url']).netloc\n",
    "    \n",
    "    visited, queue = set(), [root_url]\n",
    "    while queue and len(visited) < max_pages:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited: continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            resp = httpx.get(url, timeout=10, follow_redirects=True, verify=False)\n",
    "            if resp.status_code != 200: \n",
    "                print(f\"✗ {url}: status {resp.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            existing = list(db.t.discovered_urls.rows_where('url=?', [url], limit=1))\n",
    "            if not existing:\n",
    "                db.t.discovered_urls.insert(site_id=site_id, url=url, discovered_at=datetime.utcnow().isoformat())\n",
    "                print(f\"✓ {url} (discovered)\")\n",
    "            else:\n",
    "                print(f\"  {url} (already discovered)\")\n",
    "            \n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "            for link in get_internal_links(soup, url, root_url):\n",
    "                if urlparse(link).netloc in (domain, f\"www.{domain}\", domain.replace(\"www.\", \"\")) and link not in visited:\n",
    "                    queue.append(link)\n",
    "            \n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {url}: {e}\")\n",
    "    \n",
    "    return len(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3824da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harvgs-admin\\AppData\\Local\\Temp\\4\\ipykernel_12736\\2733547763.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  db.t.discovered_urls.insert(site_id=site_id, url=url, discovered_at=datetime.utcnow().isoformat())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com (discovered)\n",
      "✓ https://www.jea.com/About/Procurement/Look_Up_an_Invoice/ (discovered)\n",
      "✓ https://www.jea.com/About/Careers/Culture/ (discovered)\n",
      "✓ https://www.jea.com/Business_Resources/Industrial_Pretreatment/Pretreatment_Program,_Permits,_Surveys_and_Applications/ (discovered)\n",
      "✓ https://www.jea.com/Manage_My_Account/Online_Bill_Payment/ (discovered)\n",
      "✓ https://www.jea.com/About/Board_and_Management/ (discovered)\n",
      "✓ https://www.jea.com/My_Account/Residential_Forms_and_Policies/JEA_Credit_Score/ (discovered)\n",
      "✓ https://www.jea.com/My_Account/Understand_My_Bill/High_Bills/Electric_Spikes/ (discovered)\n",
      "✓ https://www.jea.com/About/Procurement/Contractor_Safety/Intrinsically_Hazardous_Work/ (discovered)\n",
      "✓ https://www.jea.com/My_Account/Residential_Forms_and_Policies/Name_Changes_and_Account_Transfers/ (discovered)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_site(db, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9c00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
