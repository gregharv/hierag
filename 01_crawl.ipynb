{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f72a4",
   "metadata": {
    "time_run": "2026-01-24T21:36:42.872358+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunks, discovered_urls, embeddings, extracts, pages, sites"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastlite import database\n",
    "\n",
    "db = database('scraper.db')\n",
    "\n",
    "sites = db.t.sites\n",
    "if sites not in db.t:\n",
    "    sites.create(id=int, root_url=str, selector=str, breadcrumb_selector=str, split_function=str, name=str, pk='id')\n",
    "    sites.create_index(['root_url'], unique=True)\n",
    "\n",
    "discovered_urls = db.t.discovered_urls\n",
    "if discovered_urls not in db.t:\n",
    "    discovered_urls.create(id=int, site_id=int, url=str, discovered_at=str, pk='id', foreign_keys=[('site_id', 'sites')])\n",
    "    discovered_urls.create_index(['url'], unique=True)\n",
    "\n",
    "pages = db.t.pages\n",
    "if pages not in db.t:\n",
    "    pages.create(id=int, site_id=int, url=str, html=str, content_hash=str, last_scraped=str, last_changed=str, pk='id', foreign_keys=[('site_id', 'sites')])\n",
    "    pages.create_index(['url'], unique=True)\n",
    "\n",
    "extracts = db.t.extracts\n",
    "if extracts not in db.t:\n",
    "    extracts.create(id=int, page_id=int, extract_index=int, text=str, pk='id', foreign_keys=[('page_id', 'pages')])\n",
    "\n",
    "chunks = db.t.chunks\n",
    "if chunks not in db.t:\n",
    "    chunks.create(id=int, extract_id=int, chunk_index=int, text=str, pk='id', foreign_keys=[('extract_id', 'extracts')])\n",
    "\n",
    "embeddings = db.t.embeddings\n",
    "if embeddings not in db.t:\n",
    "    embeddings.create(id=int, chunk_id=int, embedding=bytes, pk='id', foreign_keys=[('chunk_id', 'chunks')])\n",
    "\n",
    "db.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8fbc6",
   "metadata": {
    "time_run": "2026-01-24T21:36:42.882153+00:00"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"utils\", \"00_utils.py\")\n",
    "utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(utils)\n",
    "\n",
    "dt = db.t\n",
    "for site in utils.SITES:\n",
    "    dt.sites.insert(**site, ignore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0617c87",
   "metadata": {
    "time_run": "2026-01-24T21:36:42.889042+00:00"
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "def get_internal_links(soup, base_url, root_url):\n",
    "    \"\"\"Extract all internal links from a page that belong to the same domain as root_url.\"\"\"\n",
    "    root_parsed = urlparse(root_url)\n",
    "    root_netloc, root_scheme = root_parsed.netloc, root_parsed.scheme or 'https'\n",
    "    netloc_variants = {root_netloc, '', root_netloc[4:] if root_netloc.startswith('www.') else f'www.{root_netloc}'}\n",
    "    \n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        parsed = urlparse(urljoin(base_url, a['href']))\n",
    "        if parsed.netloc in netloc_variants:\n",
    "            clean_url = f\"{root_scheme}://{root_netloc}{parsed.path}\"\n",
    "            if not clean_url.endswith(('.pdf', '.jpg', '.png', '.gif', '.zip')):\n",
    "                links.add(clean_url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666662df",
   "metadata": {
    "time_run": "2026-01-24T21:36:42.895512+00:00"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def crawl_site(db, site_id, max_pages=10, delay=0.5):\n",
    "    \"\"\"Crawl a site to discover URLs. Only stores URLs, not HTML content.\"\"\"\n",
    "    site = db.t.sites[site_id]\n",
    "    if not site: raise ValueError(f\"No site with id {site_id}\")\n",
    "    root_url, domain = site['root_url'], urlparse(site['root_url']).netloc\n",
    "    \n",
    "    visited, queue = set(), [root_url]\n",
    "    while queue and len(visited) < max_pages:\n",
    "        url = queue.pop(0)\n",
    "        if url in visited: continue\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            resp = httpx.get(url, timeout=10, follow_redirects=True, verify=False)\n",
    "            if resp.status_code != 200: \n",
    "                print(f\"✗ {url}: status {resp.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            existing = list(db.t.discovered_urls.rows_where('url=?', [url], limit=1))\n",
    "            if not existing:\n",
    "                db.t.discovered_urls.insert(site_id=site_id, url=url, discovered_at=datetime.utcnow().isoformat())\n",
    "                print(f\"✓ {url} (discovered)\")\n",
    "            else:\n",
    "                print(f\"  {url} (already discovered)\")\n",
    "            \n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "            for link in get_internal_links(soup, url, root_url):\n",
    "                if urlparse(link).netloc in (domain, f\"www.{domain}\", domain.replace(\"www.\", \"\")) and link not in visited:\n",
    "                    queue.append(link)\n",
    "            \n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {url}: {e}\")\n",
    "    \n",
    "    return len(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824da53",
   "metadata": {
    "time_run": "2026-01-24T21:36:42.900011+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1242/2733547763.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  db.t.discovered_urls.insert(site_id=site_id, url=url, discovered_at=datetime.utcnow().isoformat())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/Outage_Center/Outage_FAQ/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/Privacy_and_Disclaimers/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/About/Procurement/Become_a_Vendor/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/About/Investor_Relations/Financial_Reports/SJRPP_Pension/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/My_Account/Residential_Forms_and_Policies/Medical_Alerts/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/Engineering_and_Construction/Water_and_Wastewater_Development/Self-Service_Center/Development_Project_Status/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/About/Careers/Culture/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/About/Procurement/Contractor_Safety/ (discovered)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ https://www.jea.com/Engineering_and_Construction/Service_Availability_Form/ (discovered)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_site(db, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9c00b",
   "metadata": {
    "time_run": "2026-01-24T21:37:06.381405+00:00"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
